# Testing Strategy for MD-Based Agentic Systems

Research document outlining a comprehensive testing strategy for specsmd, addressing the unique challenges of testing CLI tools combined with markdown-based agentic workflows.

---

## Testing Pyramid for AI Agents

Traditional testing pyramids don't account for LLM non-determinism. specsmd uses an **adapted testing pyramid** designed for AI-native systems:

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '14px'}}}%%
graph TB
    subgraph pyramid [" "]
        direction TB
        L6["ğŸ” Release Validation<br/><i>Human Review + Grok 4.1 Free</i>"]
        L5["ğŸ¤– Agent Behavior<br/><i>Promptfoo + OpenRouter Free</i>"]
        L4["ğŸ”„ E2E Workflow<br/><i>Custom Test Harness</i>"]
        L3["âš™ï¸ Integration<br/><i>BATS + Node.js</i>"]
        L2["ğŸ“‹ Schema/Contract<br/><i>JSON Schema + markdownlint</i>"]
        L1["ğŸ§ª Unit Tests<br/><i>Vitest/Jest</i>"]
    end

    L6 --> L5
    L5 --> L4
    L4 --> L3
    L3 --> L2
    L2 --> L1

    style L6 fill:#ff6b6b,stroke:#333,color:#fff
    style L5 fill:#feca57,stroke:#333,color:#333
    style L4 fill:#48dbfb,stroke:#333,color:#333
    style L3 fill:#1dd1a1,stroke:#333,color:#333
    style L2 fill:#5f27cd,stroke:#333,color:#fff
    style L1 fill:#576574,stroke:#333,color:#fff
```

### Technology Stack by Layer

| Layer | Technology | Why This Tool |
|-------|------------|---------------|
| **Unit Tests** | **Vitest** | Fast, TypeScript-native, Jest-compatible |
| **Schema/Contract** | **JSON Schema (Ajv)** + **markdownlint** | Validates YAML/MD structure without LLM |
| **Integration** | **BATS** (Bash Automated Testing) | Tests CLI commands in real shell |
| **E2E Workflow** | **Custom Node.js harness** | Orchestrates multi-agent flows |
| **Agent Behavior** | **Promptfoo** + **OpenRouter** | Declarative LLM testing, free models |
| **Release Validation** | **Grok 4.1 Fast** + **Human** | High-quality eval + manual approval |

### Characteristics by Layer

As you move UP the pyramid, tests become slower, more expensive, less deterministic, and run less frequently:

| Layer | Speed | Cost | Determinism | Volume |
|-------|-------|------|-------------|--------|
| **Unit Tests** | ~ms | Free | 100% deterministic | 100s of tests |
| **Schema/Contract** | ~ms | Free | 100% deterministic | 10s of tests |
| **Integration** | ~sec | Free | 100% deterministic | 10s of tests |
| **E2E Workflow** | ~min | Free | ~95% deterministic | 5-10 tests |
| **Agent Behavior** | ~min | Free* | ~70-85% deterministic | 50+ tests |
| **Release Validation** | ~hr | Free* | ~70-85% + human | 5-10 tests |

*100% free using OpenRouter free tier models

---

## Layer Summary

| Layer | What It Tests | Tools | Frequency | Cost | Pass Threshold |
|-------|---------------|-------|-----------|------|----------------|
| **Unit** | Installers, utilities, pure functions | Jest/Vitest | Every PR | Free | 100% |
| **Schema/Contract** | Markdown structure, YAML validity | JSON Schema, markdownlint | Every PR | Free | 100% |
| **Integration** | CLI commands, file operations | BATS, filesystem mocks | Every PR | Free | 100% |
| **E2E Workflow** | Full Inceptionâ†’Construction flow | Custom harness | On main merge | Free | 100% |
| **Agent Behavior** | LLM output quality, rubric scoring | Promptfoo + OpenRouter free | Nightly | Free* | â‰¥85% |
| **Release Validation** | Gold standard comparison, human review | Promptfoo + Grok 4.1 Free | Weekly/Release | Free* | â‰¥90% + Human approval |

*100% free using OpenRouter free tier models

---

## Frequency & Triggers

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          TEST EXECUTION SCHEDULE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  EVERY PR / PUSH (~2 min)                                              â”‚
â”‚  â”œâ”€â”€ Unit Tests                    âœ“ Must pass to merge                â”‚
â”‚  â”œâ”€â”€ Schema/Contract Tests         âœ“ Must pass to merge                â”‚
â”‚  â””â”€â”€ Integration Tests             âœ“ Must pass to merge                â”‚
â”‚                                                                         â”‚
â”‚  ON MERGE TO MAIN (~5 min)                                             â”‚
â”‚  â”œâ”€â”€ All PR tests                  âœ“ Re-run for safety                 â”‚
â”‚  â”œâ”€â”€ E2E Workflow Tests            âœ“ Full flow validation              â”‚
â”‚  â””â”€â”€ Quick Agent Eval (5 samples)  âœ“ Smoke test with free model        â”‚
â”‚                                                                         â”‚
â”‚  NIGHTLY AT 2 AM (~30 min)                                             â”‚
â”‚  â”œâ”€â”€ Full Golden Dataset           âœ“ 50+ test cases                    â”‚
â”‚  â”œâ”€â”€ Regression Check              âœ“ Compare to baseline               â”‚
â”‚  â””â”€â”€ Generate Quality Report       âœ“ Track trends                      â”‚
â”‚                                                                         â”‚
â”‚  WEEKLY / PRE-RELEASE (~1 hr)                                          â”‚
â”‚  â”œâ”€â”€ Comprehensive Evaluation      âœ“ 3x runs, averaged                 â”‚
â”‚  â”œâ”€â”€ Grok 4.1 Fast Validation      âœ“ Free high-quality judge           â”‚
â”‚  â””â”€â”€ Human Review Gate             âœ“ Manual approval required          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Developer Workflow

### Workflow 1: Adding a New Feature

When implementing a new capability (e.g., new skill, new agent behavior):

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADDING A NEW FEATURE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. WRITE SPECIFICATION
   â””â”€â”€ Create intent/unit/stories in memory-bank/
   â””â”€â”€ Schema tests will validate structure automatically

2. IMPLEMENT FEATURE
   â””â”€â”€ Add skill files, agent updates, etc.
   â””â”€â”€ Write unit tests for any new utilities

3. ADD GOLDEN EXAMPLE
   â””â”€â”€ Create input/output pair in __tests__/golden-datasets/
   â””â”€â”€ This becomes your regression baseline

4. ADD ASSERTIONS (promptfoo.yaml)
   â””â”€â”€ Add test case with:
       - vars: your input
       - assert: structure checks + llm-rubric for quality

5. RUN LOCALLY
   â””â”€â”€ `npm run test`        # Unit + schema + integration
   â””â”€â”€ `promptfoo eval`      # Agent behavior tests
   â””â”€â”€ `promptfoo view`      # Review results in UI

6. OPEN PR
   â””â”€â”€ CI runs: Unit â†’ Schema â†’ Integration
   â””â”€â”€ Must pass 100%

7. MERGE TO MAIN
   â””â”€â”€ CI runs: E2E + Quick Agent Eval
   â””â”€â”€ Feature is live

8. NIGHTLY
   â””â”€â”€ Full golden dataset runs
   â””â”€â”€ Your new test is part of regression suite
```

**Checklist for New Features**:

- [ ] Spec files follow schema (`memory-bank/intents/.../`)
- [ ] Unit tests for new utilities
- [ ] Golden dataset input/output pair added
- [ ] Promptfoo test case with assertions
- [ ] Local tests pass
- [ ] PR tests pass

---

### Workflow 2: Changing Agent Behavior

When modifying how an agent responds (e.g., changing prompt, output format):

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CHANGING AGENT BEHAVIOR                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. IDENTIFY IMPACT
   â””â”€â”€ Which golden dataset examples will be affected?
   â””â”€â”€ Which assertions might break?

2. RUN BASELINE
   â””â”€â”€ `promptfoo eval --output baseline.json`
   â””â”€â”€ Save current scores for comparison

3. MAKE CHANGES
   â””â”€â”€ Update agent/skill markdown files
   â””â”€â”€ Update prompt templates

4. RUN COMPARISON
   â””â”€â”€ `promptfoo eval --output after.json`
   â””â”€â”€ Compare scores: Are they better/same/worse?

5. UPDATE GOLDEN DATASET (if intentional change)
   â””â”€â”€ Review outputs manually
   â””â”€â”€ If new behavior is correct, update golden outputs
   â””â”€â”€ Update assertions if output format changed

6. UPDATE ASSERTIONS (if needed)
   â””â”€â”€ Adjust rubrics for new expected behavior
   â””â”€â”€ Update structural assertions

7. DOCUMENT CHANGE
   â””â”€â”€ Add to CHANGELOG
   â””â”€â”€ Note in PR: "Intentional behavior change"

8. RUN FULL EVAL
   â””â”€â”€ `npm run eval:full`
   â””â”€â”€ Ensure no unexpected regressions

9. OPEN PR
   â””â”€â”€ Include before/after comparison
   â””â”€â”€ Note which tests were updated and why
```

**Checklist for Behavior Changes**:

- [ ] Baseline captured before changes
- [ ] After scores compared to baseline
- [ ] Golden outputs updated (if intentional)
- [ ] Assertions updated (if format changed)
- [ ] No unexpected regressions
- [ ] Change documented in PR

---

### Workflow 3: Fixing a Bug

When fixing incorrect agent behavior:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FIXING A BUG                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. REPRODUCE BUG
   â””â”€â”€ Create test case that demonstrates the bug
   â””â”€â”€ Add to promptfoo.yaml with `assert` that fails

2. WRITE FAILING TEST FIRST
   â””â”€â”€ Test should fail with current code
   â””â”€â”€ `promptfoo eval` â†’ See failure

3. FIX THE BUG
   â””â”€â”€ Update agent/skill files

4. VERIFY FIX
   â””â”€â”€ `promptfoo eval` â†’ Test now passes
   â””â”€â”€ No regressions in other tests

5. ADD TO GOLDEN DATASET
   â””â”€â”€ Bug reproduction case becomes regression test
   â””â”€â”€ Prevents bug from returning

6. OPEN PR
   â””â”€â”€ Include: Bug description, test case, fix
```

**Checklist for Bug Fixes**:

- [ ] Failing test created first
- [ ] Fix makes test pass
- [ ] No regressions
- [ ] Test added to golden dataset

---

### Workflow 4: Updating Golden Dataset

When golden examples need refresh:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UPDATING GOLDEN DATASET                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. IDENTIFY STALE EXAMPLES
   â””â”€â”€ Run full eval, check similarity scores
   â””â”€â”€ Examples with <0.7 similarity may be outdated

2. REGENERATE OUTPUTS
   â””â”€â”€ Run agent with same inputs
   â””â”€â”€ Capture new outputs

3. HUMAN REVIEW
   â””â”€â”€ Compare old vs new outputs
   â””â”€â”€ Is new output actually better?
   â””â”€â”€ Get second opinion if unsure

4. UPDATE GOLDEN FILE
   â””â”€â”€ Replace __tests__/golden-datasets/.../output.md
   â””â”€â”€ Update timestamp in file

5. RE-RUN EVAL
   â””â”€â”€ `promptfoo eval`
   â””â”€â”€ Similarity should now be high

6. COMMIT WITH JUSTIFICATION
   â””â”€â”€ PR message: Why golden was updated
   â””â”€â”€ Include diff of old vs new
```

---

## Quick Commands Reference

```bash
# Development
npm run test              # Unit + Schema + Integration (fast)
npm run test:watch        # Watch mode for TDD

# Agent Evaluation
promptfoo eval            # Run agent tests
promptfoo eval --cache    # Use cached results (faster)
promptfoo view            # Open web UI to review

# CI/CD Simulation
npm run test:ci           # Full PR test suite
npm run eval:agents       # Quick agent eval (5 samples)
npm run eval:full         # Full golden dataset

# Comparison
promptfoo eval --output before.json
# make changes
promptfoo eval --output after.json
# compare in UI

# Debugging
promptfoo eval --verbose  # See full outputs
promptfoo eval --filter "intent"  # Run specific tests
```

---

## Executive Summary

specsmd presents a unique testing challenge with three distinct layers:

1. **CLI Tool Layer** (deterministic) - Commands, installers, file operations
2. **Markdown Specification Layer** (semi-deterministic) - Schema validation, structure compliance
3. **Agent Behavior Layer** (non-deterministic) - LLM-driven outputs and decisions

Traditional testing approaches fail for agent behaviors due to LLM non-determinism. This document proposes a multi-layer strategy combining specification-driven testing, behavioral assertions, and LLM-as-judge evaluation.

---

## Industry Context

### Current State of AI Agent Testing (2024-2025)

- **70% of enterprises** projected to adopt Agentic AI in testing by 2025 ([Gartner](https://www.gartner.com/en/newsroom/press-releases/2024-10-21-gartner-predicts-70-percent-of-enterprises-will-implement-ai-driven-testing-by-2027))
- **72% of QA teams** exploring AI-driven testing workflows ([Test Guild 2025](https://testguild.com/state-of-testing-2025/))
- **Terminal-Bench** (Stanford/Laude Institute, May 2025) - Evaluates agents in real CLI environments ([Terminal-Bench](https://ainativedev.io/news/8-benchmarks-shaping-the-next-generation-of-ai-agents))
- **SWT-Bench** (LogicStar AI, October 2024) - Evaluates agent test suite generation ([SWT-Bench Paper](https://arxiv.org/abs/2410.03859))
- **Specification-Driven Development (SDD)** emerging as key methodology for AI-native development ([GitHub Blog](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-using-markdown-as-a-programming-language-when-building-with-ai/))

### Key Insight

> "There remains a fundamental superiority of specifications over tests: you can derive tests from a specification, but not the other way around."
> â€” Contract-Driven Development Research

This makes specsmd's specification-first approach inherently testable.

---

## Testing Layers

### Layer 1: Specification Contract Testing

**Priority: Highest**

Since specsmd is specification-driven, markdown specs serve as contracts:

```text
memory-bank/
â”œâ”€â”€ intents/          â†’ Contract: Each intent follows schema
â”œâ”€â”€ units/            â†’ Contract: Each unit has required fields
â”œâ”€â”€ standards/        â†’ Contract: Standards follow template
â””â”€â”€ memory-bank.yaml  â†’ Contract: Schema definition
```

#### Implementation

| Component | Validation Method |
|-----------|------------------|
| `memory-bank.yaml` | JSON Schema validation |
| Intent artifacts | Required fields check |
| Unit briefs | Template compliance |
| Bolt templates | Structure validation |

#### Tools

- **JSON Schema / Ajv** - YAML/JSON schema validation
- **markdownlint** - Markdown structure enforcement
- **remark-lint** - Markdown AST validation
- **Custom validators** - specsmd-specific schema checks

#### Example Test

```javascript
describe('Intent Specification Contract', () => {
  it('should contain required sections', () => {
    const intent = loadMarkdown('memory-bank/intents/001-multi-agent/requirements.md');

    expect(intent).toHaveSection('## Problem Statement');
    expect(intent).toHaveSection('## Success Criteria');
    expect(intent).toHaveSection('## Acceptance Criteria');
  });
});
```

---

### Layer 2: CLI & Installer Testing

**Priority: High**

Traditional testing works well for deterministic CLI operations.

#### Components to Test

| Component | Testing Approach |
|-----------|------------------|
| Installers | Unit tests with mocked filesystem |
| Slash commands | Integration tests with BATS |
| File operations | Snapshot tests for generated files |
| Memory Bank CRUD | Property-based tests for schema compliance |

#### Tools

- **BATS** (Bash Automated Testing System) - CLI interaction testing
- **Jest/Vitest** - Snapshot testing for generated files
- **cram** - CLI output comparison
- **pytest** with `click.testing.CliRunner` - Python CLI testing

#### Example BATS Test

```bash
#!/usr/bin/env bats

@test "install command creates .specsmd directory" {
  run npx specsmd install
  [ "$status" -eq 0 ]
  [ -d ".specsmd" ]
  [ -f ".specsmd/memory-bank.yaml" ]
}

@test "inception agent creates intent directory" {
  run /inception intent-create --name="test-intent"
  [ "$status" -eq 0 ]
  [ -d "memory-bank/intents/test-intent" ]
}
```

---

### Layer 3: Agent Behavior Testing

**Priority: Critical (Most Complex)**

LLM outputs are non-deterministic. Traditional exact-match testing fails.

#### The Non-Determinism Challenge

> "Feed the same input to an LLM multiple times, and you might get different outputs each time. This non-determinism makes traditional unit testing approaches ineffective."

#### Tiered Approach

**Tier A: Mock LLM Responses for CI/CD**

Record and replay known-good LLM responses:

```text
__tests__/fixtures/mock-responses/
â”œâ”€â”€ inception-agent/
â”‚   â”œâ”€â”€ intent-create-success.json
â”‚   â”œâ”€â”€ requirements-elicitation.json
â”‚   â””â”€â”€ unit-generation.json
â”œâ”€â”€ construction-agent/
â”‚   â”œâ”€â”€ bolt-plan-generation.json
â”‚   â””â”€â”€ story-breakdown.json
â””â”€â”€ master-agent/
    â””â”€â”€ request-routing.json
```

Tools: `vcrpy`, `responses`, `nock`, `msw`

**Tier B: LLM-as-Judge Evaluation**

Use an LLM to evaluate agent outputs against rubrics:

```yaml
# rubrics/inception-agent-quality.yaml
evaluations:
  - name: "Intent Completeness"
    criteria: "Does the generated intent contain problem statement, goals, and success criteria?"
    threshold: 0.8

  - name: "Requirement Clarity"
    criteria: "Are requirements specific, measurable, and testable?"
    threshold: 0.75
```

Tools: **DeepEval**, **Promptfoo**, **LangWatch**

**LLM-as-Judge Provider Options**:

| Provider | Model | Cost | Best For |
|----------|-------|------|----------|
| **OpenRouter Free** | `x-ai/grok-4.1-fast:free` | $0 | **Coding agents**, agentic tasks (2M context) |
| **OpenRouter Free** | `qwen/qwen3-coder:free` | $0 | **Code-specialized** evaluation |
| **OpenRouter Free** | `meta-llama/llama-3.3-70b-instruct:free` | $0 | High-quality reasoning |
| **OpenRouter Free** | `meta-llama/llama-4-maverick:free` | $0 | General + coding (1M context) |
| **OpenRouter Free** | `google/gemma-3-27b-it:free` | $0 | Fast, good quality |
| **Claude Code** | Claude Opus 4 / Sonnet 4 | Subscription | Release validation (gold standard) |

**Recommended**: Use free OpenRouter models for CI/CD, Claude Code for final release validation

**Tier C: Semantic Similarity Tests**

Compare outputs to reference examples using embeddings:

```python
def test_intent_semantic_similarity():
    generated = inception_agent.create_intent(prompt)
    reference = load_golden_intent("user-auth-intent.md")

    similarity = cosine_similarity(embed(generated), embed(reference))
    assert similarity > 0.85, f"Semantic similarity {similarity} below threshold"
```

**Tier D: Behavioral Assertions**

Test for properties, not exact matches:

```python
def test_agent_output_structure():
    output = agent.execute(prompt)

    # Structure assertions
    assert "## Problem Statement" in output
    assert len(output) < 5000  # Reasonable length
    assert not contains_code_injection(output)

    # Semantic assertions
    assert_contains_concept(output, "authentication")
    assert_professional_tone(output)
```

---

### Layer 4: End-to-End Workflow Testing

**Priority: High**

Test complete AI-DLC flows across phases:

```text
Inception â†’ Construction â†’ Operations
```

#### Test Scenarios

| Scenario | Steps | Validation |
|----------|-------|------------|
| Happy Path | Prompt â†’ Intent â†’ Units â†’ Bolts â†’ Code | All artifacts created correctly |
| Partial Completion | Start inception, interrupt, resume | State persisted, resumable |
| Error Recovery | Invalid input â†’ Error â†’ Retry | Graceful degradation |
| Multi-Agent Coordination | Master routes â†’ Inception executes | Correct agent selection |

#### Implementation

```python
class TestAIDLCWorkflow:

    def test_full_inception_to_construction_flow(self, memory_bank):
        # Inception Phase
        result = master_agent.route("/inception intent-create --name='user-auth'")
        assert result.phase == "inception"
        assert memory_bank.has_intent("user-auth")

        # Unit elaboration
        result = inception_agent.execute("/inception units")
        assert memory_bank.intent("user-auth").has_units()

        # Transition to Construction
        result = master_agent.route("/construction bolt-plan")
        assert result.phase == "construction"
        assert memory_bank.has_bolts()
```

---

## Golden Dataset Strategy

### What is a Golden Dataset?

A curated collection of inputs and their ideal outputs for regression testing.

### Building the Dataset

| Phase | Minimum Examples | Ideal Coverage |
|-------|-----------------|----------------|
| Inception Agent | 10 | 50+ |
| Construction Agent | 10 | 50+ |
| Operations Agent | 5 | 25+ |
| Master Agent (Routing) | 20 | 100+ |

### Structure

```text
__tests__/golden-datasets/
â”œâ”€â”€ inception/
â”‚   â”œâ”€â”€ inputs/
â”‚   â”‚   â”œâ”€â”€ 001-simple-feature.txt
â”‚   â”‚   â”œâ”€â”€ 002-complex-system.txt
â”‚   â”‚   â””â”€â”€ 003-refactoring-request.txt
â”‚   â””â”€â”€ outputs/
â”‚       â”œâ”€â”€ 001-simple-feature-intent.md
â”‚       â”œâ”€â”€ 002-complex-system-intent.md
â”‚       â””â”€â”€ 003-refactoring-request-intent.md
â”œâ”€â”€ construction/
â”‚   â”œâ”€â”€ inputs/
â”‚   â””â”€â”€ outputs/
â””â”€â”€ evaluation-rubrics/
    â”œâ”€â”€ intent-quality.yaml
    â”œâ”€â”€ unit-completeness.yaml
    â””â”€â”€ bolt-validity.yaml
```

### Regression Testing Workflow

```mermaid
graph LR
    A[Code Change] --> B[Run Deterministic Tests]
    B --> C{Pass?}
    C -->|Yes| D[Run Golden Dataset Evaluation]
    C -->|No| E[Fail Fast]
    D --> F[Compare to Baselines]
    F --> G{Regression?}
    G -->|No| H[Pass]
    G -->|Yes| I[Alert + Review]
```

---

## CI/CD Integration

### Pipeline Structure

```yaml
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [main, develop]
  pull_request:
  schedule:
    - cron: '0 2 * * *'    # Nightly at 2 AM
    - cron: '0 14 * * 0'   # Weekly comprehensive (Sunday 2 PM)

jobs:
  # FAST: Every PR/Push (~2 min)
  deterministic-tests:
    name: Schema & CLI Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Schema Validation
        run: npm run test:schema
      - name: CLI Tests
        run: npm run test:cli
      - name: Snapshot Tests
        run: npm run test:snapshots

  # MEDIUM: On merge to main (~5 min) - Uses FREE OpenRouter models
  agent-evaluation:
    name: Agent Behavior Evaluation
    runs-on: ubuntu-latest
    needs: deterministic-tests
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      - name: Run Golden Dataset Evaluation
        run: npm run eval:agents
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          EVAL_MODEL: meta-llama/llama-3.1-8b-instruct:free
      - name: Check Regression Threshold
        run: npm run eval:regression-check

  # SLOW: Nightly (~30 min) - Uses FREE OpenRouter models
  nightly-evaluation:
    name: Comprehensive Agent Evaluation
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' && github.event.schedule == '0 2 * * *'
    steps:
      - uses: actions/checkout@v4
      - name: Full Golden Dataset Suite
        run: npm run eval:full
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          EVAL_MODEL: google/gemma-2-9b-it:free
      - name: Generate Evaluation Report
        run: npm run eval:report

  # COMPREHENSIVE: Weekly (~1 hr) - Uses Grok 4.1 Fast (free, 2M context)
  weekly-comprehensive:
    name: Weekly Comprehensive Evaluation
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' && github.event.schedule == '0 14 * * 0'
    steps:
      - uses: actions/checkout@v4
      - name: Full Suite with Multi-Run Averaging
        run: npm run eval:comprehensive --runs=3
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          EVAL_MODEL: x-ai/grok-4.1-fast:free
      - name: Generate Weekly Report
        run: npm run eval:weekly-report
```

### Test Categories by Trigger

| Trigger | Tests Run | Duration |
|---------|-----------|----------|
| Every PR | Schema, CLI, Snapshots | ~2 min |
| Merge to main | + Quick agent eval | ~5 min |
| Nightly | Full golden dataset | ~30 min |
| Release | Full suite + human review | ~1 hour |

---

## Recommended Test Directory Structure

```text
specsmd/
â”œâ”€â”€ __tests__/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ installers/
â”‚   â”‚   â”‚   â”œâ”€â”€ claude-code-installer.test.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ cursor-installer.test.ts
â”‚   â”‚   â”‚   â””â”€â”€ copilot-installer.test.ts
â”‚   â”‚   â”œâ”€â”€ schema-validation/
â”‚   â”‚   â”‚   â”œâ”€â”€ memory-bank-schema.test.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ intent-schema.test.ts
â”‚   â”‚   â”‚   â””â”€â”€ unit-schema.test.ts
â”‚   â”‚   â””â”€â”€ template-generation/
â”‚   â”‚       â”œâ”€â”€ bolt-template.test.ts
â”‚   â”‚       â””â”€â”€ artifact-templates.test.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”‚   â”œâ”€â”€ install.bats
â”‚   â”‚   â”‚   â”œâ”€â”€ inception-commands.bats
â”‚   â”‚   â”‚   â””â”€â”€ construction-commands.bats
â”‚   â”‚   â””â”€â”€ memory-bank/
â”‚   â”‚       â”œâ”€â”€ crud-operations.test.ts
â”‚   â”‚       â””â”€â”€ file-system-state.test.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ e2e/
â”‚   â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”‚   â”œâ”€â”€ full-aidlc-flow.test.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ inception-phase.test.ts
â”‚   â”‚   â”‚   â””â”€â”€ construction-phase.test.ts
â”‚   â”‚   â””â”€â”€ agent-chains/
â”‚   â”‚       â””â”€â”€ multi-agent-coordination.test.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ golden-datasets/
â”‚   â”‚   â”‚   â”œâ”€â”€ inception/
â”‚   â”‚   â”‚   â”œâ”€â”€ construction/
â”‚   â”‚   â”‚   â””â”€â”€ operations/
â”‚   â”‚   â”œâ”€â”€ rubrics/
â”‚   â”‚   â”‚   â”œâ”€â”€ intent-quality.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ unit-completeness.yaml
â”‚   â”‚   â”‚   â””â”€â”€ bolt-validity.yaml
â”‚   â”‚   â””â”€â”€ regression/
â”‚   â”‚       â”œâ”€â”€ baselines/
â”‚   â”‚       â””â”€â”€ reports/
â”‚   â”‚
â”‚   â””â”€â”€ fixtures/
â”‚       â”œâ”€â”€ mock-responses/
â”‚       â”‚   â”œâ”€â”€ inception-agent/
â”‚       â”‚   â”œâ”€â”€ construction-agent/
â”‚       â”‚   â””â”€â”€ master-agent/
â”‚       â””â”€â”€ memory-bank-states/
â”‚           â”œâ”€â”€ empty-project/
â”‚           â”œâ”€â”€ inception-complete/
â”‚           â””â”€â”€ construction-in-progress/
â”‚
â”œâ”€â”€ jest.config.js
â”œâ”€â”€ vitest.config.ts
â””â”€â”€ promptfoo.yaml
```

---

## Tools Summary

| Layer | Tools | Purpose |
|-------|-------|---------|
| Schema Validation | JSON Schema, Ajv, Zod | Validate YAML/JSON structures |
| Markdown Linting | markdownlint, remark-lint | Enforce markdown structure |
| CLI Testing | BATS, pytest, cram | Test command-line interactions |
| Snapshot Testing | Jest, Vitest | Verify generated file content |
| LLM Mocking | vcrpy, responses, nock, msw | Record/replay LLM responses |
| LLM Evaluation | DeepEval, Promptfoo, LangWatch | Evaluate agent output quality |
| Regression Testing | Evidently AI, Giskard, custom | Detect quality degradation |
| E2E Agent Testing | Checksum, custom harness | Full workflow validation |

---

## LLM-as-Judge Implementation

### Provider Strategy

Use different providers based on context and cost requirements:

```yaml
# evaluation-config.yaml
providers:
  pr_checks:
    type: openrouter
    model: x-ai/grok-4.1-fast:free
    temperature: 0
    # Free, fast, optimized for agentic coding (2M context)

  code_review:
    type: openrouter
    model: qwen/qwen3-coder:free
    temperature: 0
    # Free, code-specialized evaluation

  nightly:
    type: openrouter
    model: meta-llama/llama-3.3-70b-instruct:free
    temperature: 0
    # Free, high-quality reasoning for comprehensive eval

  release:
    type: openrouter
    model: x-ai/grok-4.1-fast:free
    temperature: 0
    runs: 3  # Multi-run averaging
    # Best free model: 2M context, excellent for agentic/coding evaluation
```

### OpenRouter Integration (Recommended for CI/CD)

OpenRouter provides OpenAI-compatible API with free model access:

```python
# tests/evaluation/judge.py
from openai import OpenAI
import os

class OpenRouterJudge:
    def __init__(self, model: str = "meta-llama/llama-3.1-8b-instruct:free"):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY"),
            default_headers={"HTTP-Referer": "https://specs.md"}
        )
        self.model = model

    def evaluate(self, output: str, rubric: str) -> dict:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": f"""Evaluate this agent output against the rubric.

Output:
{output}

Rubric:
{rubric}

Return JSON: {{"score": 0.0-1.0, "pass": true/false, "reasoning": "..."}}"""
            }],
            temperature=0,
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
```

### Promptfoo Configuration with OpenRouter

```yaml
# promptfoo.yaml
providers:
  - id: openrouter:meta-llama/llama-3.1-8b-instruct:free
    config:
      temperature: 0

defaultTest:
  assert:
    - type: llm-rubric
      value: |
        Evaluate if the output:
        1. Contains required sections (problem, goals, criteria)
        2. Uses professional language
        3. Is actionable and specific
        Return PASS if score >= 0.8

tests:
  - description: "Intent creation quality"
    vars:
      prompt: "Create an intent for user authentication"
    assert:
      - type: llm-rubric
        provider: openrouter:meta-llama/llama-3.1-8b-instruct:free
        value: "Does this intent have clear requirements?"
```

### Cost Comparison

| Provider | Model | Cost/1M tokens | Rate Limit | Best For |
|----------|-------|----------------|------------|----------|
| OpenRouter | Grok 4.1 Fast (free) | $0 | ~20 req/min | **Coding agents** |
| OpenRouter | Qwen3 Coder (free) | $0 | ~20 req/min | **Code evaluation** |
| OpenRouter | Llama 3.3 70B (free) | $0 | ~20 req/min | Reasoning |
| OpenRouter | Llama 4 Maverick (free) | $0 | ~20 req/min | General (1M ctx) |
| OpenRouter | Gemma 3 27B (free) | $0 | ~20 req/min | Fast fallback |
| OpenRouter | **Grok 4.1 Fast (free)** | $0 | ~20 req/min | **Releases** (2M ctx) |

**Recommendation**: Use `grok-4.1-fast:free` for everything - coding agents, release validation (2M context handles full evaluation). Use `qwen3-coder:free` for code-specific assertions. **Entire testing stack is $0.**

---

## Handling Non-Determinism

### Strategies

1. **Set Temperature to 0** for tests requiring deterministic outputs
2. **Use Semantic Similarity** instead of exact match (threshold: 0.85+)
3. **Run Multiple Evaluations** and average scores for critical tests
4. **LLM-as-Judge** for subjective quality criteria
5. **Behavioral Assertions** that check properties, not strings

### Example: Handling Flaky Tests

```python
@pytest.mark.flaky(max_runs=3, min_passes=2)
def test_agent_generates_valid_intent():
    """
    Run 3 times, pass if 2+ succeed.
    Accounts for LLM non-determinism.
    """
    output = inception_agent.create_intent(prompt)

    # Semantic check, not exact match
    assert semantic_similarity(output, reference) > 0.85
    assert has_required_sections(output)
```

---

## Human-in-the-Loop Validation

### When Human Review is Required

| Scenario | Trigger | Review Type |
|----------|---------|-------------|
| Phase Transition | Inception â†’ Construction | Approval gate |
| Operations Deployment | Before production deploy | Manual verification |
| Golden Dataset Updates | New baseline proposed | Quality review |
| Regression Detected | Score drop > 10% | Investigation |

### Implementation

```yaml
# In CI/CD workflow
- name: Human Approval Gate
  if: steps.eval.outputs.phase_transition == 'true'
  uses: trstringer/manual-approval@v1
  with:
    approvers: team-leads
    message: "Review inception artifacts before construction phase"
```

---

## Metrics and Monitoring

### Key Metrics to Track

| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| Schema Validation Pass Rate | 100% | < 100% |
| CLI Test Pass Rate | 100% | < 100% |
| Agent Output Quality Score | > 0.85 | < 0.80 |
| Semantic Similarity (avg) | > 0.90 | < 0.85 |
| Regression from Baseline | 0% | > 5% |
| Test Execution Time | < 5 min (PR) | > 10 min |

### Dashboard

Track over time:

- Quality scores per agent
- Regression trends
- Test flakiness rate
- Coverage of golden dataset

---

## Implementation Roadmap

### Phase 1: Foundation (Week 1-2)

- [ ] Set up test directory structure
- [ ] Implement schema validation for `memory-bank.yaml`
- [ ] Add markdownlint configuration
- [ ] Create first BATS tests for CLI commands

### Phase 2: Golden Dataset (Week 3-4)

- [ ] Create 10 golden examples per agent
- [ ] Implement semantic similarity testing
- [ ] Set up Promptfoo or DeepEval
- [ ] Define evaluation rubrics

### Phase 3: CI/CD Integration (Week 5-6)

- [ ] Configure GitHub Actions workflow
- [ ] Set up regression baseline tracking
- [ ] Implement human approval gates
- [ ] Create evaluation dashboards

### Phase 4: Expansion (Ongoing)

- [ ] Expand golden dataset to 50+ examples
- [ ] Add property-based testing
- [ ] Implement continuous monitoring
- [ ] Refine rubrics based on production data

---

## References

- [Specification-Driven Development - GitHub Blog](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-using-markdown-as-a-programming-language-when-building-with-ai/)
- [LLM Regression Testing Tutorial - Evidently AI](https://www.evidentlyai.com/blog/llm-regression-testing-tutorial)
- [DeepEval Documentation](https://deepeval.com/docs)
- [Promptfoo - LLM Testing Framework](https://github.com/promptfoo/promptfoo)
- [Building Effective Agents - Anthropic](https://www.anthropic.com/engineering/building-effective-agents)
- [Terminal-Bench - Stanford/Laude Institute](https://ainativedev.io/news/8-benchmarks-shaping-the-next-generation-of-ai-agents)
- [Contract-Driven Development Research](https://link.springer.com/chapter/10.1007/978-3-540-71289-3_2)

---

*Document created: 2025-12-09*
*Last updated: 2025-12-10*
*Status: Research / Draft*
